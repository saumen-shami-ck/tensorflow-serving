# Use CK Rocky Linux as base image
FROM us-docker.pkg.dev/ck-corp-artifact-registry/ckdocker-gold/security/rocky-linux:prod

# Install necessary dependencies
RUN dnf -y update && dnf -y install \
    automake \
    ca-certificates \
    curl \
    gcc \
    gcc-c++ \
    git \
    libtool \
    make \
    openssl-devel \
    patch \
    unzip \
    zlib-devel \
    which \
    python38 \
    java-1.8.0-openjdk-devel \
    java-1.8.0-openjdk-headless

RUN pip3 install --index-url https://artifactory.corp.creditkarma.com/artifactory/api/pypi/pypi/simple \
    future>=0.17.1 \
    grpcio \
    h5py \
    keras_applications>=1.0.8 \
    keras_preprocessing>=1.1.0 \
    mock \
    numpy \
    portpicker \
    requests \
    --ignore-installed setuptools \
    --ignore-installed six>=1.12.0


# Install Bazel
RUN dnf install curl gpg -y

RUN curl -fsSL https://bazel.build/bazel-release.pub.gpg | gpg --dearmor >bazel-archive-keyring.gpg

RUN mv bazel-archive-keyring.gpg /etc/pki/rpm-gpg/

ENV BAZEL_VERSION=4.2.1

RUN curl -fLO "https://github.com/bazelbuild/bazel/releases/download/${BAZEL_VERSION}/bazel-${BAZEL_VERSION}-installer-linux-x86_64.sh"

RUN chmod +x "bazel-${BAZEL_VERSION}-installer-linux-x86_64.sh"

RUN "./bazel-${BAZEL_VERSION}-installer-linux-x86_64.sh"

RUN bazel version

# Clone the TensorFlow Serving repo down to /tensorflow-serving
WORKDIR /tensorflow-serving
RUN git clone --branch=r2.7 --depth=1 https://github.com/tensorflow/serving . && \
    git submodule update --init --recursive && \
    echo "/usr/local/lib/bazel/bin" >> /etc/environment

# Building TensorFlow Serving
ARG TF_SERVING_BUILD_OPTIONS="--local_resources 2048,.5,1.0"
RUN echo "building with options: ${TF_SERVING_BUILD_OPTIONS}"
RUN bazel build --color=yes --curses=yes \
    ${TF_SERVING_BUILD_OPTIONS} \
    tensorflow_serving/model_servers:tensorflow_model_server && \
    cp bazel-bin/tensorflow_serving/model_servers/tensorflow_model_server /usr/local/bin/

# Expose ports
# gRPC
EXPOSE 8500
# REST
EXPOSE 8501

# Set where models should be stored in the container
ENV MODEL_BASE_PATH=/models
RUN mkdir -p ${MODEL_BASE_PATH}

# The only required piece is the model name in order to differentiate endpoints
ENV MODEL_NAME=model

# Create a script that runs the model server so we can customize how we want it run
# Set the entrypoint to be that script so it runs when the container starts
COPY tensorflow_serving/model_servers/run_in_docker.sh /usr/bin/run_in_docker.sh
RUN chmod +x /usr/bin/run_in_docker.sh
ENTRYPOINT ["/usr/bin/run_in_docker.sh"]
